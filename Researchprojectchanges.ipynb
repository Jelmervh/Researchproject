{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into pandas dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Administrative', 'Administrative_Duration', 'Informational',\n",
      "       'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
      "       'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month',\n",
      "       'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n",
      "       'Weekend', 'Revenue'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "df = pd.read_csv(\"online_shoppers_intention_numbers.csv\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "#splitting the Class variable and the features\n",
    "X = df.drop(columns=['Revenue'])\n",
    "Y = df['Revenue']\n",
    "\n",
    "#Making different datasets based on the top 10 features for testing purposes\n",
    "datasets = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 32342) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by Duc, data nomalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "       Administrative  Administrative_Duration  Informational  \\\n",
      "2637               14               230.106944              0   \n",
      "9415                5               158.700000              0   \n",
      "11579              11               449.750000              0   \n",
      "6437                1                22.200000              2   \n",
      "2198                0                 0.000000              0   \n",
      "11826               0                 0.000000              4   \n",
      "5663                0                 0.000000              0   \n",
      "9553                1                 3.000000              0   \n",
      "2084                1                 5.000000              0   \n",
      "\n",
      "       Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
      "2637                     0.00              52              2059.109203   \n",
      "9415                     0.00              51              1364.266667   \n",
      "11579                    0.00              52              1786.109649   \n",
      "6437                    44.40              37               400.800000   \n",
      "2198                     0.00               2               162.000000   \n",
      "11826                  117.25              96              5286.208333   \n",
      "5663                     0.00              45              2347.333333   \n",
      "9553                     0.00              13               543.000000   \n",
      "2084                     0.00              13               545.571429   \n",
      "\n",
      "       BounceRates  ExitRates  PageValues  SpecialDay  Month  \\\n",
      "2637      0.003747   0.008451    7.610431         0.0      5   \n",
      "9415      0.007692   0.011987    0.000000         0.0     11   \n",
      "11579     0.000000   0.017119   40.656712         0.0     11   \n",
      "6437      0.005128   0.002564    0.000000         0.0     10   \n",
      "2198      0.000000   0.050000    0.000000         0.0      5   \n",
      "11826     0.011000   0.024119    0.000000         0.0     11   \n",
      "5663      0.004545   0.015909    0.000000         0.0     11   \n",
      "9553      0.000000   0.008333    0.000000         0.0     12   \n",
      "2084      0.013333   0.020784   26.120154         0.0      3   \n",
      "\n",
      "       OperatingSystems  Browser  Region  TrafficType  VisitorType  Weekend  \n",
      "2637                  2        2       1           13            1        0  \n",
      "9415                  3        2       3            2            1        1  \n",
      "11579                 1        1       1            2            1        0  \n",
      "6437                  2       10       1            2            1        0  \n",
      "2198                  2        4       1            4            1        0  \n",
      "11826                 2        2       1            1            1        1  \n",
      "5663                  2        4       1            1            1        0  \n",
      "9553                  2        2       1            2            0        0  \n",
      "2084                  2        2       3            2            1        1  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original data\")\n",
    "print(X_train[0:9][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data\n",
      "[[5.18518519e-01 6.77034040e-02 0.00000000e+00 0.00000000e+00\n",
      "  7.37588652e-02 3.21868975e-02 1.87353650e-02 4.22556200e-02\n",
      "  2.64293485e-02 0.00000000e+00 3.00000000e-01 1.42857143e-01\n",
      "  8.33333333e-02 0.00000000e+00 6.31578947e-01 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [1.85185185e-01 4.66936374e-02 0.00000000e+00 0.00000000e+00\n",
      "  7.23404255e-02 2.13254894e-02 3.84615400e-02 5.99358950e-02\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e-01 2.85714286e-01\n",
      "  8.33333333e-02 2.50000000e-01 5.26315789e-02 5.00000000e-01\n",
      "  1.00000000e+00]\n",
      " [4.07407407e-01 1.32328062e-01 0.00000000e+00 0.00000000e+00\n",
      "  7.37588652e-02 2.79195140e-02 0.00000000e+00 8.55932200e-02\n",
      "  1.41191793e-01 0.00000000e+00 9.00000000e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 5.26315789e-02 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [3.70370370e-02 6.53181317e-03 8.33333333e-02 1.74160333e-02\n",
      "  5.24822695e-02 6.26509196e-03 2.56410250e-02 1.28205150e-02\n",
      "  0.00000000e+00 0.00000000e+00 8.00000000e-01 1.42857143e-01\n",
      "  7.50000000e-01 0.00000000e+00 5.26315789e-02 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.83687943e-03 2.53229765e-03 0.00000000e+00 2.50000000e-01\n",
      "  0.00000000e+00 0.00000000e+00 3.00000000e-01 1.42857143e-01\n",
      "  2.50000000e-01 0.00000000e+00 1.57894737e-01 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.66666667e-01 4.59916646e-02\n",
      "  1.36170213e-01 8.26311910e-02 5.50000000e-02 1.20595240e-01\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e-01 1.42857143e-01\n",
      "  8.33333333e-02 0.00000000e+00 0.00000000e+00 5.00000000e-01\n",
      "  1.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.38297872e-02 3.66922635e-02 2.27272750e-02 7.95454550e-02\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e-01 1.42857143e-01\n",
      "  2.50000000e-01 0.00000000e+00 0.00000000e+00 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [3.70370370e-02 8.82677455e-04 0.00000000e+00 0.00000000e+00\n",
      "  1.84397163e-02 8.48788657e-03 0.00000000e+00 4.16666650e-02\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.42857143e-01\n",
      "  8.33333333e-02 0.00000000e+00 5.26315789e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [3.70370370e-02 1.47112909e-03 0.00000000e+00 0.00000000e+00\n",
      "  1.84397163e-02 8.52808177e-03 6.66666650e-02 1.03921570e-01\n",
      "  9.07095333e-02 0.00000000e+00 1.00000000e-01 1.42857143e-01\n",
      "  8.33333333e-02 2.50000000e-01 5.26315789e-02 5.00000000e-01\n",
      "  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaled data\")\n",
    "print(X_train_scaled[0:9][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "       ProductRelated  ProductRelated_Duration  BounceRates  ExitRates  \\\n",
      "2637               52              2059.109203     0.003747   0.008451   \n",
      "9415               51              1364.266667     0.007692   0.011987   \n",
      "11579              52              1786.109649     0.000000   0.017119   \n",
      "6437               37               400.800000     0.005128   0.002564   \n",
      "2198                2               162.000000     0.000000   0.050000   \n",
      "...               ...                      ...          ...        ...   \n",
      "8136               22              2281.583333     0.029697   0.043687   \n",
      "12078              45              1896.966667     0.011765   0.027451   \n",
      "7077               26               710.000000     0.003846   0.017521   \n",
      "4891               21               204.166667     0.028571   0.048413   \n",
      "4679               15               537.200000     0.011765   0.023529   \n",
      "\n",
      "       PageValues  \n",
      "2637     7.610431  \n",
      "9415     0.000000  \n",
      "11579   40.656712  \n",
      "6437     0.000000  \n",
      "2198     0.000000  \n",
      "...           ...  \n",
      "8136     0.000000  \n",
      "12078    7.521155  \n",
      "7077     0.000000  \n",
      "4891     0.000000  \n",
      "4679    27.664000  \n",
      "\n",
      "[9864 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "datasets.update({\"17\" : { \"X_train\": X_train, \"X_test\": X_test}})\n",
    "datasets.update({\"10\" : { \"X_train\": X_train.drop(columns =['Month', 'TrafficType', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser']),\n",
    "                                                 \"X_test\" : X_test.drop(columns =['Month', 'TrafficType', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser'])}})\n",
    "datasets.update({\"9\": { \"X_train\": datasets[\"10\"][\"X_train\"].drop(columns =['SpecialDay']), \n",
    "                       \"X_test\": datasets[\"10\"][\"X_test\"].drop(columns = ['SpecialDay'])}})\n",
    "datasets.update({\"8\": { \"X_train\": datasets[\"9\"][\"X_train\"].drop(columns =['Administrative_Duration']), \n",
    "                       \"X_test\": datasets[\"9\"][\"X_test\"].drop(columns = ['Administrative_Duration'])}})\n",
    "datasets.update({\"7\": { \"X_train\": datasets[\"8\"][\"X_train\"].drop(columns =['Informational']), \n",
    "                       \"X_test\": datasets[\"8\"][\"X_test\"].drop(columns = ['Informational'])}})\n",
    "datasets.update({\"6\": { \"X_train\": datasets[\"7\"][\"X_train\"].drop(columns =['VisitorType']), \n",
    "                       \"X_test\": datasets[\"7\"][\"X_test\"].drop(columns = ['VisitorType'])}})\n",
    "datasets.update({\"5\": { \"X_train\": datasets[\"6\"][\"X_train\"].drop(columns =['Administrative']), \n",
    "                       \"X_test\": datasets[\"6\"][\"X_test\"].drop(columns = ['Administrative'])}})\n",
    "datasets.update({\"4\": { \"X_train\": datasets[\"5\"][\"X_train\"].drop(columns =['BounceRates']), \n",
    "                       \"X_test\": datasets[\"5\"][\"X_test\"].drop(columns = ['BounceRates'])}})\n",
    "datasets.update({\"3\": { \"X_train\": datasets[\"4\"][\"X_train\"].drop(columns =['ProductRelated_Duration']), \n",
    "                       \"X_test\": datasets[\"4\"][\"X_test\"].drop(columns = ['ProductRelated_Duration'])}})\n",
    "datasets.update({\"2\": { \"X_train\": datasets[\"3\"][\"X_train\"].drop(columns =['ProductRelated']), \n",
    "                       \"X_test\": datasets[\"3\"][\"X_test\"].drop(columns = ['ProductRelated'])}})\n",
    "datasets.update({\"1\": { \"X_train\": datasets[\"2\"][\"X_train\"].drop(columns =['ExitRates']), \n",
    "                       \"X_test\": datasets[\"2\"][\"X_test\"].drop(columns = ['ExitRates'])}})\n",
    "\n",
    "\n",
    "#for dataset in datasets:\n",
    "#    print(datasets[dataset])\n",
    "\n",
    "print(Y.values)\n",
    "print(datasets[\"5\"][\"X_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size (9864, 17) (9864,)\n",
      "Train test size (2466, 17) (2466,)\n"
     ]
    }
   ],
   "source": [
    "# added by Duc for debugging\n",
    "print(\"Train data size\", X_train.shape, y_train.shape)\n",
    "print(\"Train test size\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples per class in train set: Counter({0: 8357, 1: 1507})\n",
      "number of examples per class in test set: Counter({0: 2065, 1: 401})\n"
     ]
    }
   ],
   "source": [
    "# added by Duc\n",
    "import collections\n",
    "print(\"number of examples per class in train set:\", collections.Counter(y_train))\n",
    "print(\"number of examples per class in test set:\", collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "\n",
    "def tsne_plot(x, y):\n",
    "\n",
    "    sns.set(style =\"whitegrid\") \n",
    "      \n",
    "    tsne = TSNE(n_components = 2, random_state = 0) \n",
    "      \n",
    "    # Reducing the dimensionality of the data \n",
    "    X_transformed = tsne.fit_transform(x) \n",
    "  \n",
    "    plt.figure(figsize =(12, 8)) \n",
    "      \n",
    "    # Building the scatter plot \n",
    "    plt.scatter(X_transformed[np.where(y == 0), 0],  \n",
    "                X_transformed[np.where(y == 0), 1], \n",
    "                marker ='o', color ='y', linewidth ='1', \n",
    "                alpha = 0.8, label ='Non-Buyer') \n",
    "    plt.scatter(X_transformed[np.where(y == 1), 0], \n",
    "                X_transformed[np.where(y == 1), 1], \n",
    "                marker ='o', color ='k', linewidth ='1', \n",
    "                alpha = 0.8, label ='Buyer') \n",
    "  \n",
    "    # Specifying the location of the legend \n",
    "    plt.legend(loc ='best') \n",
    "      \n",
    "    # Plotting the reduced data \n",
    "    plt.show() \n",
    "\n",
    "#tsne_plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "#for dataset in datasets:\n",
    "        \n",
    "        #scaler = MinMaxScaler()\n",
    "        #datasets[dataset].update({\"X_train\": scaler.fit_transform(datasets[dataset][\"X_train\"]), \n",
    "                                  #\"X_test\" : scaler.fit_transform(datasets[dataset][\"X_test\"])})\n",
    "    \n",
    "    \n",
    "   # datasets[dataset] = scaler.fit_transform(datasets[dataset])\n",
    "    \n",
    "#scaled_datasets['10'] = X_scaled.drop(columns = ['Month', 'Traffictype', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser'])\n",
    "#scaled_datasets['9'] = X10_scaled.drop(columns =['SpecialDay'])\n",
    "#scaled_datasets['8'] = X9_scaled.drop(columns = ['Administrative_Duration'])\n",
    "#scaled_datasets['7'] = X8_scaled.drop(columns = ['Informational'])\n",
    "#scaled_datasets['6'] = X7_scaled.drop(columns = ['VisitorType'])\n",
    "#scaled_datasets['5'] = X6_scaled.drop(columns = ['Administrative'])\n",
    "#scaled_datasets['4'] = X5_scaled.drop(columns = ['BounceRates'])\n",
    "#scaled_datasets['3'] = X4_scaled.drop(columns = ['ProductRelated_Duration'])\n",
    "#scaled_datasets['2'] = X3_scaled.drop(columns = ['ProductRelated'])\n",
    "#scaled_datasets['1'] = X2_scaled.drop(columns = ['ExitRates'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tsne_plot(X_scaled, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of features: 17\n",
      "Accuracy : 0.8398215733982157\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 394    7]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 394\n",
      "True positives: 7\n",
      "\n",
      "amount of features: 10\n",
      "Accuracy : 0.8402270884022709\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 393    8]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 393\n",
      "True positives: 8\n",
      "\n",
      "amount of features: 9\n",
      "Accuracy : 0.8402270884022709\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 393    8]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 393\n",
      "True positives: 8\n",
      "\n",
      "amount of features: 8\n",
      "Accuracy : 0.8406326034063261\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 392    9]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 392\n",
      "True positives: 9\n",
      "\n",
      "amount of features: 7\n",
      "Accuracy : 0.8406326034063261\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 392    9]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 392\n",
      "True positives: 9\n",
      "\n",
      "amount of features: 6\n",
      "Accuracy : 0.8406326034063261\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 392    9]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 392\n",
      "True positives: 9\n",
      "\n",
      "amount of features: 5\n",
      "Accuracy : 0.8406326034063261\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 392    9]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 392\n",
      "True positives: 9\n",
      "\n",
      "amount of features: 4\n",
      "Accuracy : 0.8406326034063261\n",
      "Confusion Matrix: \n",
      "[[2064    1]\n",
      " [ 392    9]]\n",
      "True negatives: 2064\n",
      "False positives: 1\n",
      "False negatives: 392\n",
      "True positives: 9\n",
      "\n",
      "amount of features: 3\n",
      "Accuracy : 0.8888888888888888\n",
      "Confusion Matrix: \n",
      "[[1983   82]\n",
      " [ 192  209]]\n",
      "True negatives: 1983\n",
      "False positives: 82\n",
      "False negatives: 192\n",
      "True positives: 209\n",
      "\n",
      "amount of features: 2\n",
      "Accuracy : 0.8896999188969992\n",
      "Confusion Matrix: \n",
      "[[1961  104]\n",
      " [ 168  233]]\n",
      "True negatives: 1961\n",
      "False positives: 104\n",
      "False negatives: 168\n",
      "True positives: 233\n",
      "\n",
      "amount of features: 1\n",
      "Accuracy : 0.8896999188969992\n",
      "Confusion Matrix: \n",
      "[[1961  104]\n",
      " [ 168  233]]\n",
      "True negatives: 1961\n",
      "False positives: 104\n",
      "False negatives: 168\n",
      "True positives: 233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "\n",
    "# Splitting dataset into train set and test set.\n",
    "def makepredictions(datasets):\n",
    "    \n",
    "    for dataset in datasets:\n",
    "    \n",
    "     #   X_train, X_test, y_train, y_test = train_test_split(datasets[dataset], Y, test_size = 0.2, random_state = 32342) \n",
    "\n",
    "\n",
    "        svmclf = SVC() \n",
    "        svmclf.fit(datasets[dataset][\"X_train\"], y_train) \n",
    "\n",
    "        y_pred_svmclf = svmclf.predict(datasets[dataset][\"X_test\"]) \n",
    "  \n",
    "        # Performance\n",
    "        print('amount of features: ' + dataset)\n",
    "        print('Accuracy : '+str(accuracy_score(y_test, y_pred_svmclf))) \n",
    "        print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_svmclf)))\n",
    "        # incorrect order of output\n",
    "#         tn, fn, fp, tp = confusion_matrix(y_test, y_pred_svmclf).ravel()\n",
    "        # the right order is as follows\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_svmclf).ravel()\n",
    "    \n",
    "        print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n",
    "makepredictions(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7891 samples, validate on 1973 samples\n",
      "Epoch 1/10\n",
      "7891/7891 [==============================] - 0s 55us/step - loss: 0.0735 - val_loss: 0.0525\n",
      "Epoch 2/10\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 0.0474 - val_loss: 0.0412\n",
      "Epoch 3/10\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 0.0413 - val_loss: 0.0399\n",
      "Epoch 4/10\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 0.0402 - val_loss: 0.0388\n",
      "Epoch 5/10\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 0.0392 - val_loss: 0.0378\n",
      "Epoch 6/10\n",
      "7891/7891 [==============================] - 0s 3us/step - loss: 0.0383 - val_loss: 0.0367\n",
      "Epoch 7/10\n",
      "7891/7891 [==============================] - 0s 3us/step - loss: 0.0371 - val_loss: 0.0355\n",
      "Epoch 8/10\n",
      "7891/7891 [==============================] - 0s 3us/step - loss: 0.0360 - val_loss: 0.0344\n",
      "Epoch 9/10\n",
      "7891/7891 [==============================] - 0s 3us/step - loss: 0.0350 - val_loss: 0.0337\n",
      "Epoch 10/10\n",
      "7891/7891 [==============================] - 0s 3us/step - loss: 0.0345 - val_loss: 0.0342\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'17'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bf420e40925a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m#y_buyer = np.ones(buyer_hidden_rep.shape[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m#encoded_y = np.append(y_nonbuyer, y_buyer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mencoded_X_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_representation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"17\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mencoded_X_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_representation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"17\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '17'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense \n",
    "from keras.models import Model, Sequential \n",
    "from keras import regularizers \n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras import backend as K\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#X_scaled = scaler.fit_transform(X)\n",
    "#X_nonbuyer_scaled = X_scaled[Y == 0] \n",
    "#X_buyer_scaled = X_scaled[Y == 1]\n",
    "\n",
    "\n",
    "input_layer = Input(shape =(X.shape[1], )) \n",
    "\n",
    "#Encoder network Deep\n",
    "encoded_deep = Dense(100, activation ='tanh')(input_layer) \n",
    "encoded_deep = Dense(50, activation ='tanh')(encoded_deep) \n",
    "encoded_deep = Dense(25, activation ='tanh')(encoded_deep) \n",
    "encoded_deep = Dense(12, activation ='tanh')(encoded_deep) \n",
    "encoded_deep = Dense(6, activation ='relu')(encoded_deep) \n",
    "\n",
    "#Decoder network deep\n",
    "decoded_deep = Dense(12, activation ='tanh')(encoded_deep) \n",
    "decoded_deep = Dense(25, activation ='tanh')(decoded_deep) \n",
    "decoded_deep = Dense(50, activation ='tanh')(decoded_deep) \n",
    "decoded_deep = Dense(100, activation ='tanh')(decoded_deep) \n",
    "\n",
    "output_layer_deep = Dense(X.shape[1], activation ='relu')(decoded_deep) \n",
    "\n",
    "\n",
    "\n",
    "    # Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer_deep) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "\n",
    "    # Training the Auto-encoder network \n",
    "autoencoder.fit(X_train_scaled,X_train_scaled,  \n",
    "                    batch_size = 3000, epochs = 10,  \n",
    "                    shuffle = True, validation_split = 0.2) \n",
    "\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "    # Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "    #nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "    #buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "\n",
    "    # Combining the encoded points into a single table  \n",
    "    #encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "    #y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "    #y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "    #encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "encoded_X_train = hidden_representation.predict(datasets[\"17\"][\"X_train\"]) \n",
    "\n",
    "encoded_X_test = hidden_representation.predict(datasets[\"17\"][\"X_test\"])\n",
    "\n",
    "\n",
    "classifier = SVC() \n",
    "classifier.fit(encoded_X_train, y_train) \n",
    "\n",
    "    # Storing the predictions of the linear model \n",
    "y_pred_classifier = classifier.predict(encoded_X_test) \n",
    "\n",
    "\n",
    "    # Plotting the encoded points \n",
    "    #tsne_plot(encoded_X, encoded_y) \n",
    "\n",
    "\n",
    "    # Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test, y_pred_classifier))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_classifier)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classifier).ravel()\n",
    "    \n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7891 samples, validate on 1973 samples\n",
      "Epoch 1/10\n",
      "7891/7891 [==============================] - 0s 58us/step - loss: 6.7973 - val_loss: 4.4013\n",
      "Epoch 2/10\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 5.8673 - val_loss: 3.8692\n",
      "Epoch 3/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 5.1807 - val_loss: 3.4705\n",
      "Epoch 4/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 4.6983 - val_loss: 3.1745\n",
      "Epoch 5/10\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 4.3214 - val_loss: 2.9311\n",
      "Epoch 6/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 3.9982 - val_loss: 2.7395\n",
      "Epoch 7/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 3.7653 - val_loss: 2.5669\n",
      "Epoch 8/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 3.5268 - val_loss: 2.4320\n",
      "Epoch 9/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 3.3614 - val_loss: 2.3090\n",
      "Epoch 10/10\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 3.2217 - val_loss: 2.2146\n",
      "amount of features: \n",
      "Accuracy : 0.8669910786699108\n",
      "Confusion Matrix: \n",
      "[[2045   20]\n",
      " [ 308   93]]\n",
      "True negatives: 2045\n",
      "False positives: 20\n",
      "False negatives: 308\n",
      "True positives: 93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer_sparse = Input(shape =(X.shape[1], )) \n",
    "\n",
    "#Encoder network Sparsity constraint\n",
    "encoded_sparse = Dense(100, activation ='tanh',\n",
    "activity_regularizer = regularizers.l1(10e-5))(input_layer_sparse) \n",
    "encoded_sparse = Dense(50, activation ='tanh',\n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded_sparse) \n",
    "encoded_sparse = Dense(25, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded_sparse) \n",
    "encoded_sparse = Dense(12, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded_sparse) \n",
    "encoded_sparse = Dense(6, activation ='relu')(encoded_sparse) \n",
    "\n",
    "#Decoder network Sparsity constraint\n",
    "decoded_sparse = Dense(12, activation ='tanh')(encoded_sparse) \n",
    "decoded_sparse = Dense(25, activation ='tanh')(decoded_sparse) \n",
    "decoded_sparse = Dense(50, activation ='tanh')(decoded_sparse) \n",
    "decoded_sparse = Dense(100, activation ='tanh')(decoded_sparse) \n",
    "\n",
    "output_layer_sparse = Dense(X.shape[1], activation ='relu')(decoded_sparse) \n",
    "\n",
    "\n",
    "\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer_sparse, output_layer_sparse) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "\n",
    "# Training the Auto-encoder network \n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,  \n",
    "                    batch_size = 3000, epochs = 10,  \n",
    "                    shuffle = True, validation_split = 0.2) \n",
    "\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "    # Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "    #nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "    #buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "\n",
    "    # Combining the encoded points into a single table  \n",
    "    #encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "    #y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "    #y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "    #encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "encoded_X_train = hidden_representation.predict(X_train_scaled) \n",
    "\n",
    "encoded_X_test = hidden_representation.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "classifier = SVC() \n",
    "classifier.fit(encoded_X_train, y_train) \n",
    "\n",
    "    # Storing the predictions of the linear model \n",
    "y_pred_classifier = classifier.predict(encoded_X_test) \n",
    "\n",
    "\n",
    "    # Plotting the encoded points \n",
    "    #tsne_plot(encoded_X, encoded_y) \n",
    "\n",
    "\n",
    "    # Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test, y_pred_classifier))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_classifier)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classifier).ravel()\n",
    "    \n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12330\n",
      "16\n",
      "16\n",
      "Train data size (9864, 16) (9864,)\n",
      "Train test size (2466, 16) (2466,)\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 4, 4, 16)          80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 2, 2, 1)           65        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 1, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 1, 1, 1)           5         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling (None, 2, 2, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 2, 2, 16)          80        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 4, 4, 1)           65        \n",
      "=================================================================\n",
      "Total params: 295\n",
      "Trainable params: 295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7891 samples, validate on 1973 samples\n",
      "Epoch 1/2\n",
      "7891/7891 [==============================] - 0s 43us/step - loss: 0.1787 - val_loss: 0.1587\n",
      "Epoch 2/2\n",
      "7891/7891 [==============================] - 0s 17us/step - loss: 0.1107 - val_loss: 0.0670\n",
      "Train data size (9864, 1, 1, 1) (9864,)\n",
      "Train test size (2466, 1, 1, 1) (2466,)\n",
      "9864\n",
      "amount of features: \n",
      "Accuracy : 0.8373884833738848\n",
      "Confusion Matrix: \n",
      "[[2065    0]\n",
      " [ 401    0]]\n",
      "True negatives: 2065\n",
      "False positives: 0\n",
      "False negatives: 401\n",
      "True positives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Encoder network Convolutional\n",
    "print(X.shape[0])\n",
    "X_train_conv = X_train.drop(columns =['OperatingSystems'])\n",
    "X_test_conv = X_test.drop(columns = ['OperatingSystems'])\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_conv)\n",
    "X_train_scaled_conv = scaler.transform(X_train_conv)\n",
    "X_test_scaled_conv = scaler.transform(X_test_conv)\n",
    "print(X_train_scaled_conv.shape[1])\n",
    "print(X_test_scaled_conv.shape[1])\n",
    "print(\"Train data size\", X_train_scaled_conv.shape, y_train.shape)\n",
    "print(\"Train test size\", X_test_scaled_conv.shape, y_test.shape)\n",
    "X_train_scaled_conv = np.reshape(X_train_scaled_conv, (len(X_train_scaled_conv), 4, 4, 1))  # adapt this if using `channels_first` image data format\n",
    "X_test_scaled_conv = np.reshape(X_test_scaled_conv, (len(X_test_scaled_conv), 4, 4, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "input_layer = Input(shape =(4,4,1))\n",
    "\n",
    "x = Conv2D(16, (2, 2), activation='relu', padding='same')(input_layer)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(1, (2, 2), activation='relu', padding='same')(x)\n",
    "encoded_conv = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(1, (2, 2), activation='relu', padding='same')(encoded_conv)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded_conv = Conv2D(1, (2, 2), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, decoded_conv) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "autoencoder.summary()\n",
    "    # Training the Auto-encoder network \n",
    "autoencoder.fit(X_train_scaled_conv, X_train_scaled_conv,  \n",
    "                    batch_size = 500, epochs = 2,  \n",
    "                    shuffle = True, validation_split = 0.2) \n",
    "\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "    # Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "    #nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "    #buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "\n",
    "    # Combining the encoded points into a single table  \n",
    "    #encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "    #y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "    #y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "    #encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "encoded_X_train = hidden_representation.predict(X_train_scaled_conv) \n",
    "\n",
    "encoded_X_test = hidden_representation.predict(X_test_scaled_conv)\n",
    "\n",
    "\n",
    "classifier = SVC() \n",
    "print(\"Train data size\", encoded_X_train.shape, y_train.shape)\n",
    "print(\"Train test size\", encoded_X_test.shape, y_test.shape)\n",
    "print(len(encoded_X_train))\n",
    "encoded_X_train = np.reshape(encoded_X_train, (len(encoded_X_train), 1)) \n",
    "encoded_X_test = np.reshape(encoded_X_test, (len(encoded_X_test), 1)) \n",
    "classifier.fit(encoded_X_train, y_train) \n",
    "\n",
    "    # Storing the predictions of the linear model \n",
    "y_pred_classifier = classifier.predict(encoded_X_test) \n",
    "\n",
    "\n",
    "    # Plotting the encoded points \n",
    "    #tsne_plot(encoded_X, encoded_y) \n",
    "\n",
    "\n",
    "    # Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test, y_pred_classifier))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_classifier)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classifier).ravel()\n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
