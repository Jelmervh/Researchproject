{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into pandas dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Administrative', 'Administrative_Duration', 'Informational',\n",
      "       'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
      "       'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month',\n",
      "       'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n",
      "       'Weekend', 'Revenue'],\n",
      "      dtype='object')\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"online_shoppers_intention_numbers.csv\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "#splitting the Class variable and the features\n",
    "X = df.drop(columns=['Revenue'])\n",
    "Y = df['Revenue']\n",
    "\n",
    "#Making different datasets based on the top 10 features for testing purposes\n",
    "datasets = {}\n",
    "\n",
    "datasets[str(len(df.columns))] = X\n",
    "datasets['10'] = X.drop(columns =['Month', 'TrafficType', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser'])\n",
    "datasets['9'] = datasets['10'].drop(columns =['SpecialDay'])\n",
    "datasets['8'] = datasets['9'].drop(columns = ['Administrative_Duration'])\n",
    "datasets['7'] = datasets['8'].drop(columns = ['Informational'])\n",
    "datasets['6'] = datasets['7'].drop(columns = ['VisitorType'])\n",
    "datasets['5'] = datasets['6'].drop(columns = ['Administrative'])\n",
    "datasets['4'] = datasets['5'].drop(columns = ['BounceRates'])\n",
    "datasets['3'] = datasets['4'].drop(columns = ['ProductRelated_Duration'])\n",
    "datasets['2'] = datasets['3'].drop(columns = ['ProductRelated'])\n",
    "datasets['1'] = datasets['2'].drop(columns = ['ExitRates'])\n",
    "\n",
    "#for dataset in datasets:\n",
    "#    print(datasets[dataset])\n",
    "\n",
    "print(Y.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "def tsne_plot(x, y):\n",
    "\n",
    "    sns.set(style =\"whitegrid\") \n",
    "      \n",
    "    tsne = TSNE(n_components = 2, random_state = 0) \n",
    "      \n",
    "    # Reducing the dimensionality of the data \n",
    "    X_transformed = tsne.fit_transform(x) \n",
    "  \n",
    "    plt.figure(figsize =(12, 8)) \n",
    "      \n",
    "    # Building the scatter plot \n",
    "    plt.scatter(X_transformed[np.where(y == 0), 0],  \n",
    "                X_transformed[np.where(y == 0), 1], \n",
    "                marker ='o', color ='y', linewidth ='1', \n",
    "                alpha = 0.8, label ='Non-Buyer') \n",
    "    plt.scatter(X_transformed[np.where(y == 1), 0], \n",
    "                X_transformed[np.where(y == 1), 1], \n",
    "                marker ='o', color ='k', linewidth ='1', \n",
    "                alpha = 0.8, label ='Buyer') \n",
    "  \n",
    "    # Specifying the location of the legend \n",
    "    plt.legend(loc ='best') \n",
    "      \n",
    "    # Plotting the reduced data \n",
    "    plt.show() \n",
    "\n",
    "#tsne_plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    scaler = MinMaxScaler()\n",
    "    datasets[dataset] = scaler.fit_transform(datasets[dataset])\n",
    "    \n",
    "#scaled_datasets['10'] = X_scaled.drop(columns = ['Month', 'Traffictype', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser'])\n",
    "#scaled_datasets['9'] = X10_scaled.drop(columns =['SpecialDay'])\n",
    "#scaled_datasets['8'] = X9_scaled.drop(columns = ['Administrative_Duration'])\n",
    "#scaled_datasets['7'] = X8_scaled.drop(columns = ['Informational'])\n",
    "#scaled_datasets['6'] = X7_scaled.drop(columns = ['VisitorType'])\n",
    "#scaled_datasets['5'] = X6_scaled.drop(columns = ['Administrative'])\n",
    "#scaled_datasets['4'] = X5_scaled.drop(columns = ['BounceRates'])\n",
    "#scaled_datasets['3'] = X4_scaled.drop(columns = ['ProductRelated_Duration'])\n",
    "#scaled_datasets['2'] = X3_scaled.drop(columns = ['ProductRelated'])\n",
    "#scaled_datasets['1'] = X2_scaled.drop(columns = ['ExitRates'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tsne_plot(X_scaled, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of features: 18\n",
      "Accuracy : 0.8763179237631792\n",
      "Confusion Matrix: \n",
      "[[2033   32]\n",
      " [ 273  128]]\n",
      "True negatives: 2033\n",
      "False positives: 32\n",
      "False negatives: 273\n",
      "True positives: 128\n",
      "\n",
      "amount of features: 10\n",
      "Accuracy : 0.8868613138686131\n",
      "Confusion Matrix: \n",
      "[[2022   43]\n",
      " [ 236  165]]\n",
      "True negatives: 2022\n",
      "False positives: 43\n",
      "False negatives: 236\n",
      "True positives: 165\n",
      "\n",
      "amount of features: 9\n",
      "Accuracy : 0.8868613138686131\n",
      "Confusion Matrix: \n",
      "[[2021   44]\n",
      " [ 235  166]]\n",
      "True negatives: 2021\n",
      "False positives: 44\n",
      "False negatives: 235\n",
      "True positives: 166\n",
      "\n",
      "amount of features: 8\n",
      "Accuracy : 0.8872668288726683\n",
      "Confusion Matrix: \n",
      "[[2021   44]\n",
      " [ 234  167]]\n",
      "True negatives: 2021\n",
      "False positives: 44\n",
      "False negatives: 234\n",
      "True positives: 167\n",
      "\n",
      "amount of features: 7\n",
      "Accuracy : 0.8856447688564477\n",
      "Confusion Matrix: \n",
      "[[2020   45]\n",
      " [ 237  164]]\n",
      "True negatives: 2020\n",
      "False positives: 45\n",
      "False negatives: 237\n",
      "True positives: 164\n",
      "\n",
      "amount of features: 6\n",
      "Accuracy : 0.8864557988645579\n",
      "Confusion Matrix: \n",
      "[[2018   47]\n",
      " [ 233  168]]\n",
      "True negatives: 2018\n",
      "False positives: 47\n",
      "False negatives: 233\n",
      "True positives: 168\n",
      "\n",
      "amount of features: 5\n",
      "Accuracy : 0.8864557988645579\n",
      "Confusion Matrix: \n",
      "[[2003   62]\n",
      " [ 218  183]]\n",
      "True negatives: 2003\n",
      "False positives: 62\n",
      "False negatives: 218\n",
      "True positives: 183\n",
      "\n",
      "amount of features: 4\n",
      "Accuracy : 0.8880778588807786\n",
      "Confusion Matrix: \n",
      "[[1993   72]\n",
      " [ 204  197]]\n",
      "True negatives: 1993\n",
      "False positives: 72\n",
      "False negatives: 204\n",
      "True positives: 197\n",
      "\n",
      "amount of features: 3\n",
      "Accuracy : 0.8884833738848338\n",
      "Confusion Matrix: \n",
      "[[1994   71]\n",
      " [ 204  197]]\n",
      "True negatives: 1994\n",
      "False positives: 71\n",
      "False negatives: 204\n",
      "True positives: 197\n",
      "\n",
      "amount of features: 2\n",
      "Accuracy : 0.8876723438767234\n",
      "Confusion Matrix: \n",
      "[[1983   82]\n",
      " [ 195  206]]\n",
      "True negatives: 1983\n",
      "False positives: 82\n",
      "False negatives: 195\n",
      "True positives: 206\n",
      "\n",
      "amount of features: 1\n",
      "Accuracy : 0.8896999188969992\n",
      "Confusion Matrix: \n",
      "[[1961  104]\n",
      " [ 168  233]]\n",
      "True negatives: 1961\n",
      "False positives: 104\n",
      "False negatives: 168\n",
      "True positives: 233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "\n",
    "# Splitting dataset into train set and test set.\n",
    "def makepredictions(datasets):\n",
    "    \n",
    "    for dataset in datasets:\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(datasets[dataset], Y, test_size = 0.2, random_state = 32342) \n",
    "\n",
    "\n",
    "        svmclf = SVC() \n",
    "        svmclf.fit(X_train, y_train) \n",
    "\n",
    "        y_pred_svmclf = svmclf.predict(X_test) \n",
    "  \n",
    "        # Performance\n",
    "        print('amount of features: ' + dataset)\n",
    "        print('Accuracy : '+str(accuracy_score(y_test, y_pred_svmclf))) \n",
    "        print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_svmclf)))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_svmclf).ravel()\n",
    "        print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n",
    "makepredictions(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8337 samples, validate on 2085 samples\n",
      "Epoch 1/10\n",
      "8337/8337 [==============================] - 0s 49us/step - loss: 3.2603 - val_loss: 2.8986\n",
      "Epoch 2/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 2.4118 - val_loss: 2.3104\n",
      "Epoch 3/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 2.0276 - val_loss: 2.0003\n",
      "Epoch 4/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.7992 - val_loss: 1.7913\n",
      "Epoch 5/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.6223 - val_loss: 1.6295\n",
      "Epoch 6/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.4849 - val_loss: 1.5040\n",
      "Epoch 7/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.4096 - val_loss: 1.3992\n",
      "Epoch 8/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.3182 - val_loss: 1.3143\n",
      "Epoch 9/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.2332 - val_loss: 1.2421\n",
      "Epoch 10/10\n",
      "8337/8337 [==============================] - 0s 4us/step - loss: 1.1842 - val_loss: 1.1842\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense \n",
    "from keras.models import Model, Sequential \n",
    "from keras import regularizers \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_nonbuyer_scaled = X_scaled[Y == 0] \n",
    "X_buyer_scaled = X_scaled[Y == 1]\n",
    "\n",
    "\n",
    "input_layer = Input(shape =(X.shape[1], )) \n",
    "\n",
    "#Encoder network \n",
    "encoded = Dense(100, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(input_layer) \n",
    "encoded = Dense(50, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = Dense(25, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = Dense(12, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = Dense(6, activation ='relu')(encoded) \n",
    "\n",
    "#Decoder network \n",
    "decoded = Dense(12, activation ='tanh')(encoded) \n",
    "decoded = Dense(25, activation ='tanh')(decoded) \n",
    "decoded = Dense(50, activation ='tanh')(decoded) \n",
    "decoded = Dense(100, activation ='tanh')(decoded) \n",
    "\n",
    "# Building the Output Layer \n",
    "output_layer = Dense(X.shape[1], activation ='relu')(decoded) \n",
    "\n",
    "\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "  \n",
    "# Training the Auto-encoder network \n",
    "autoencoder.fit(X_nonbuyer_scaled, X_nonbuyer_scaled,  \n",
    "                batch_size = 1500, epochs = 10,  \n",
    "                shuffle = True, validation_split = 0.2) \n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "  \n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "  \n",
    "# Plotting the encoded points \n",
    "#tsne_plot(encoded_X, encoded_y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908\n",
      "10422\n",
      "12330\n",
      "12330\n",
      "amount of features: \n",
      "Accuracy : 0.8913219789132197\n",
      "Confusion Matrix: \n",
      "[[2038   46]\n",
      " [ 222  160]]\n",
      "True negatives: 2038\n",
      "False positives: 46\n",
      "False negatives: 222\n",
      "True positives: 160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "print(len(y_buyer))\n",
    "print(len(y_nonbuyer))\n",
    "print(len(encoded_y))\n",
    "# Splitting the encoded data for linear classification \n",
    "X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(encoded_X, encoded_y, test_size = 0.2, random_state = 32342)\n",
    "print(len(encoded_y))\n",
    "# Building the logistic regression model \n",
    "lrclf = SVC() \n",
    "lrclf.fit(X_train_encoded, y_train_encoded) \n",
    "  \n",
    "# Storing the predictions of the linear model \n",
    "y_pred_lrclf = lrclf.predict(X_test_encoded) \n",
    "\n",
    "# Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test_encoded, y_pred_lrclf))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test_encoded,y_pred_lrclf)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_encoded, y_pred_lrclf).ravel()\n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10422, 17)\n",
      "(1908, 17)\n",
      "Train on 8337 samples, validate on 2085 samples\n",
      "Epoch 1/10\n",
      "8337/8337 [==============================] - 1s 71us/step - loss: 0.0767 - val_loss: 0.0664\n",
      "Epoch 2/10\n",
      "8337/8337 [==============================] - 1s 68us/step - loss: 0.0589 - val_loss: 0.0594\n",
      "Epoch 3/10\n",
      "8337/8337 [==============================] - 1s 68us/step - loss: 0.0527 - val_loss: 0.0531\n",
      "Epoch 4/10\n",
      "8337/8337 [==============================] - 1s 69us/step - loss: 0.0487 - val_loss: 0.0497\n",
      "Epoch 5/10\n",
      "8337/8337 [==============================] - 1s 69us/step - loss: 0.0465 - val_loss: 0.0528\n",
      "Epoch 6/10\n",
      "8337/8337 [==============================] - 1s 67us/step - loss: 0.0449 - val_loss: 0.0608\n",
      "Epoch 7/10\n",
      "8337/8337 [==============================] - 1s 67us/step - loss: 0.0441 - val_loss: 0.0466\n",
      "Epoch 8/10\n",
      "8337/8337 [==============================] - 1s 66us/step - loss: 0.0435 - val_loss: 0.0549\n",
      "Epoch 9/10\n",
      "8337/8337 [==============================] - 1s 67us/step - loss: 0.0432 - val_loss: 0.0596\n",
      "Epoch 10/10\n",
      "8337/8337 [==============================] - 1s 70us/step - loss: 0.0429 - val_loss: 0.0455\n",
      "amount of features: \n",
      "Accuracy : 0.8450932684509327\n",
      "Confusion Matrix: \n",
      "[[2084    0]\n",
      " [ 382    0]]\n",
      "True negatives: 2084\n",
      "False positives: 0\n",
      "False negatives: 382\n",
      "True positives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Denoising Autoencoder\n",
    "\n",
    "#loading only images and not their labels\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_nonbuyer_scaled = X_scaled[Y == 0] \n",
    "X_buyer_scaled = X_scaled[Y == 1]\n",
    "\n",
    "\n",
    "X_nonbuyer_scaled = X_nonbuyer_scaled.astype('float32')/255\n",
    "X_buyer_scaled = X_buyer_scaled.astype('float32')/255\n",
    "\n",
    "X_nonbuyer_scaled = X_nonbuyer_scaled.reshape(len(X_nonbuyer_scaled), np.prod(X_nonbuyer_scaled.shape[1:]))\n",
    "X_buyer_scaled = X_buyer_scaled.reshape(len(X_buyer_scaled), np.prod(X_buyer_scaled.shape[1:]))\n",
    "X_nonbuyer_noisy = X_nonbuyer_scaled + np.random.normal(loc=0.0, scale=0.5, size=X_nonbuyer_scaled.shape)\n",
    "X_nonbuyer_noisy = np.clip(X_nonbuyer_noisy, 0., 1.)\n",
    "X_buyer_noisy = X_buyer_scaled + np.random.normal(loc=0.0, scale=0.5, size=X_buyer_scaled.shape)\n",
    "X_buyer_noisy = np.clip(X_buyer_noisy, 0., 1.)\n",
    "print(X_nonbuyer_noisy.shape)\n",
    "print(X_buyer_noisy.shape)\n",
    "\n",
    "\n",
    "autoencoder.fit(X_nonbuyer_noisy, X_nonbuyer_noisy,  \n",
    "                batch_size = 16, epochs = 10,  \n",
    "                shuffle = True, validation_split = 0.2) \n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_noisy) \n",
    "buyer_hidden_rep = hidden_representation.predict(X_buyer_noisy) \n",
    "  \n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "\n",
    "X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(encoded_X, encoded_y, test_size = 0.2, random_state = 32342)\n",
    "\n",
    "lrclf.fit(X_train_encoded, y_train_encoded) \n",
    "y_pred_lrclf = lrclf.predict(X_test_encoded) \n",
    "\n",
    "\n",
    "# Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test_encoded, y_pred_lrclf))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test_encoded,y_pred_lrclf)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_encoded, y_pred_lrclf).ravel()\n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
