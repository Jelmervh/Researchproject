{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into pandas dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "df = pd.read_csv(\"online_shoppers_intention_numbers.csv\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "#splitting the Class variable and the features\n",
    "X = df.drop(columns=['Revenue'])\n",
    "Y = df['Revenue']\n",
    "\n",
    "#Making different datasets based on the top 10 features for testing purposes\n",
    "datasets = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 32342) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by Duc, data nomalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "       Administrative  Administrative_Duration  Informational  \\\n",
      "2637               14               230.106944              0   \n",
      "9415                5               158.700000              0   \n",
      "11579              11               449.750000              0   \n",
      "6437                1                22.200000              2   \n",
      "2198                0                 0.000000              0   \n",
      "11826               0                 0.000000              4   \n",
      "5663                0                 0.000000              0   \n",
      "9553                1                 3.000000              0   \n",
      "2084                1                 5.000000              0   \n",
      "\n",
      "       Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
      "2637                     0.00              52              2059.109203   \n",
      "9415                     0.00              51              1364.266667   \n",
      "11579                    0.00              52              1786.109649   \n",
      "6437                    44.40              37               400.800000   \n",
      "2198                     0.00               2               162.000000   \n",
      "11826                  117.25              96              5286.208333   \n",
      "5663                     0.00              45              2347.333333   \n",
      "9553                     0.00              13               543.000000   \n",
      "2084                     0.00              13               545.571429   \n",
      "\n",
      "       BounceRates  ExitRates  PageValues  SpecialDay  Month  \\\n",
      "2637      0.003747   0.008451    7.610431         0.0      5   \n",
      "9415      0.007692   0.011987    0.000000         0.0     11   \n",
      "11579     0.000000   0.017119   40.656712         0.0     11   \n",
      "6437      0.005128   0.002564    0.000000         0.0     10   \n",
      "2198      0.000000   0.050000    0.000000         0.0      5   \n",
      "11826     0.011000   0.024119    0.000000         0.0     11   \n",
      "5663      0.004545   0.015909    0.000000         0.0     11   \n",
      "9553      0.000000   0.008333    0.000000         0.0     12   \n",
      "2084      0.013333   0.020784   26.120154         0.0      3   \n",
      "\n",
      "       OperatingSystems  Browser  Region  TrafficType  VisitorType  Weekend  \n",
      "2637                  2        2       1           13            1        0  \n",
      "9415                  3        2       3            2            1        1  \n",
      "11579                 1        1       1            2            1        0  \n",
      "6437                  2       10       1            2            1        0  \n",
      "2198                  2        4       1            4            1        0  \n",
      "11826                 2        2       1            1            1        1  \n",
      "5663                  2        4       1            1            1        0  \n",
      "9553                  2        2       1            2            0        0  \n",
      "2084                  2        2       3            2            1        1  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original data\")\n",
    "print(X_train[0:9][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data\n",
      "[[5.18518519e-01 6.77034040e-02 0.00000000e+00 0.00000000e+00\n",
      "  7.37588652e-02 3.21868975e-02 1.87353650e-02 4.22556200e-02\n",
      "  2.64293485e-02 0.00000000e+00 3.00000000e-01 1.42857143e-01\n",
      "  8.33333333e-02 0.00000000e+00 6.31578947e-01 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [1.85185185e-01 4.66936374e-02 0.00000000e+00 0.00000000e+00\n",
      "  7.23404255e-02 2.13254894e-02 3.84615400e-02 5.99358950e-02\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e-01 2.85714286e-01\n",
      "  8.33333333e-02 2.50000000e-01 5.26315789e-02 5.00000000e-01\n",
      "  1.00000000e+00]\n",
      " [4.07407407e-01 1.32328062e-01 0.00000000e+00 0.00000000e+00\n",
      "  7.37588652e-02 2.79195140e-02 0.00000000e+00 8.55932200e-02\n",
      "  1.41191793e-01 0.00000000e+00 9.00000000e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 5.26315789e-02 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [3.70370370e-02 6.53181317e-03 8.33333333e-02 1.74160333e-02\n",
      "  5.24822695e-02 6.26509196e-03 2.56410250e-02 1.28205150e-02\n",
      "  0.00000000e+00 0.00000000e+00 8.00000000e-01 1.42857143e-01\n",
      "  7.50000000e-01 0.00000000e+00 5.26315789e-02 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.83687943e-03 2.53229765e-03 0.00000000e+00 2.50000000e-01\n",
      "  0.00000000e+00 0.00000000e+00 3.00000000e-01 1.42857143e-01\n",
      "  2.50000000e-01 0.00000000e+00 1.57894737e-01 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.66666667e-01 4.59916646e-02\n",
      "  1.36170213e-01 8.26311910e-02 5.50000000e-02 1.20595240e-01\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e-01 1.42857143e-01\n",
      "  8.33333333e-02 0.00000000e+00 0.00000000e+00 5.00000000e-01\n",
      "  1.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.38297872e-02 3.66922635e-02 2.27272750e-02 7.95454550e-02\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e-01 1.42857143e-01\n",
      "  2.50000000e-01 0.00000000e+00 0.00000000e+00 5.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [3.70370370e-02 8.82677455e-04 0.00000000e+00 0.00000000e+00\n",
      "  1.84397163e-02 8.48788657e-03 0.00000000e+00 4.16666650e-02\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.42857143e-01\n",
      "  8.33333333e-02 0.00000000e+00 5.26315789e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [3.70370370e-02 1.47112909e-03 0.00000000e+00 0.00000000e+00\n",
      "  1.84397163e-02 8.52808177e-03 6.66666650e-02 1.03921570e-01\n",
      "  9.07095333e-02 0.00000000e+00 1.00000000e-01 1.42857143e-01\n",
      "  8.33333333e-02 2.50000000e-01 5.26315789e-02 5.00000000e-01\n",
      "  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaled data\")\n",
    "print(X_train_scaled[0:9][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Administrative', 'Administrative_Duration', 'Informational',\n",
      "       'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
      "       'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month',\n",
      "       'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n",
      "       'Weekend', 'Revenue'],\n",
      "      dtype='object')\n",
      "[0 0 0 ... 0 0 0]\n",
      "       ProductRelated  ProductRelated_Duration  BounceRates  ExitRates  \\\n",
      "2637               52              2059.109203     0.003747   0.008451   \n",
      "9415               51              1364.266667     0.007692   0.011987   \n",
      "11579              52              1786.109649     0.000000   0.017119   \n",
      "6437               37               400.800000     0.005128   0.002564   \n",
      "2198                2               162.000000     0.000000   0.050000   \n",
      "...               ...                      ...          ...        ...   \n",
      "8136               22              2281.583333     0.029697   0.043687   \n",
      "12078              45              1896.966667     0.011765   0.027451   \n",
      "7077               26               710.000000     0.003846   0.017521   \n",
      "4891               21               204.166667     0.028571   0.048413   \n",
      "4679               15               537.200000     0.011765   0.023529   \n",
      "\n",
      "       PageValues  \n",
      "2637     7.610431  \n",
      "9415     0.000000  \n",
      "11579   40.656712  \n",
      "6437     0.000000  \n",
      "2198     0.000000  \n",
      "...           ...  \n",
      "8136     0.000000  \n",
      "12078    7.521155  \n",
      "7077     0.000000  \n",
      "4891     0.000000  \n",
      "4679    27.664000  \n",
      "\n",
      "[9864 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "datasets.update({\"17\" : { \"X_train\": X_train, \"X_test\": X_test}})\n",
    "datasets.update({\"10\" : { \"X_train\": X_train.drop(columns =['Month', 'TrafficType', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser']),\n",
    "                                                 \"X_test\" : X_test.drop(columns =['Month', 'TrafficType', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser'])}})\n",
    "datasets.update({\"9\": { \"X_train\": datasets[\"10\"][\"X_train\"].drop(columns =['SpecialDay']), \n",
    "                       \"X_test\": datasets[\"10\"][\"X_test\"].drop(columns = ['SpecialDay'])}})\n",
    "datasets.update({\"8\": { \"X_train\": datasets[\"9\"][\"X_train\"].drop(columns =['Administrative_Duration']), \n",
    "                       \"X_test\": datasets[\"9\"][\"X_test\"].drop(columns = ['Administrative_Duration'])}})\n",
    "datasets.update({\"7\": { \"X_train\": datasets[\"8\"][\"X_train\"].drop(columns =['Informational']), \n",
    "                       \"X_test\": datasets[\"8\"][\"X_test\"].drop(columns = ['Informational'])}})\n",
    "datasets.update({\"6\": { \"X_train\": datasets[\"7\"][\"X_train\"].drop(columns =['VisitorType']), \n",
    "                       \"X_test\": datasets[\"7\"][\"X_test\"].drop(columns = ['VisitorType'])}})\n",
    "datasets.update({\"5\": { \"X_train\": datasets[\"6\"][\"X_train\"].drop(columns =['Administrative']), \n",
    "                       \"X_test\": datasets[\"6\"][\"X_test\"].drop(columns = ['Administrative'])}})\n",
    "datasets.update({\"4\": { \"X_train\": datasets[\"5\"][\"X_train\"].drop(columns =['BounceRates']), \n",
    "                       \"X_test\": datasets[\"5\"][\"X_test\"].drop(columns = ['BounceRates'])}})\n",
    "datasets.update({\"3\": { \"X_train\": datasets[\"4\"][\"X_train\"].drop(columns =['ProductRelated_Duration']), \n",
    "                       \"X_test\": datasets[\"4\"][\"X_test\"].drop(columns = ['ProductRelated_Duration'])}})\n",
    "datasets.update({\"2\": { \"X_train\": datasets[\"3\"][\"X_train\"].drop(columns =['ProductRelated']), \n",
    "                       \"X_test\": datasets[\"3\"][\"X_test\"].drop(columns = ['ProductRelated'])}})\n",
    "datasets.update({\"1\": { \"X_train\": datasets[\"2\"][\"X_train\"].drop(columns =['ExitRates']), \n",
    "                       \"X_test\": datasets[\"2\"][\"X_test\"].drop(columns = ['ExitRates'])}})\n",
    "\n",
    "\n",
    "#for dataset in datasets:\n",
    "#    print(datasets[dataset])\n",
    "\n",
    "print(Y.values)\n",
    "print(datasets[\"5\"][\"X_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size (9864, 17) (9864,)\n",
      "Train test size (2466, 17) (2466,)\n"
     ]
    }
   ],
   "source": [
    "# added by Duc for debugging\n",
    "print(\"Train data size\", X_train.shape, y_train.shape)\n",
    "print(\"Train test size\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples per class in train set: Counter({0: 8357, 1: 1507})\n",
      "number of examples per class in test set: Counter({0: 2065, 1: 401})\n"
     ]
    }
   ],
   "source": [
    "# added by Duc\n",
    "import collections\n",
    "print(\"number of examples per class in train set:\", collections.Counter(y_train))\n",
    "print(\"number of examples per class in test set:\", collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "\n",
    "def tsne_plot(x, y):\n",
    "\n",
    "    sns.set(style =\"whitegrid\") \n",
    "      \n",
    "    tsne = TSNE(n_components = 2, random_state = 0) \n",
    "      \n",
    "    # Reducing the dimensionality of the data \n",
    "    X_transformed = tsne.fit_transform(x) \n",
    "  \n",
    "    plt.figure(figsize =(12, 8)) \n",
    "      \n",
    "    # Building the scatter plot \n",
    "    plt.scatter(X_transformed[np.where(y == 0), 0],  \n",
    "                X_transformed[np.where(y == 0), 1], \n",
    "                marker ='o', color ='y', linewidth ='1', \n",
    "                alpha = 0.8, label ='Non-Buyer') \n",
    "    plt.scatter(X_transformed[np.where(y == 1), 0], \n",
    "                X_transformed[np.where(y == 1), 1], \n",
    "                marker ='o', color ='k', linewidth ='1', \n",
    "                alpha = 0.8, label ='Buyer') \n",
    "  \n",
    "    # Specifying the location of the legend \n",
    "    plt.legend(loc ='best') \n",
    "      \n",
    "    # Plotting the reduced data \n",
    "    plt.show() \n",
    "\n",
    "#tsne_plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "# for dataset in datasets:\n",
    "        \n",
    "#         scaler = MinMaxScaler()\n",
    "#         datasets[dataset].update({\"X_train\": scaler.fit_transform(datasets[dataset][\"X_train\"]), \n",
    "#                                   \"X_test\" : scaler.fit_transform(datasets[dataset][\"X_test\"])})\n",
    "    \n",
    "    \n",
    "#    # datasets[dataset] = scaler.fit_transform(datasets[dataset])\n",
    "    \n",
    "# #scaled_datasets['10'] = X_scaled.drop(columns = ['Month', 'Traffictype', 'Informational_Duration', 'OperatingSystems', 'Weekend','Region','Browser'])\n",
    "# #scaled_datasets['9'] = X10_scaled.drop(columns =['SpecialDay'])\n",
    "# #scaled_datasets['8'] = X9_scaled.drop(columns = ['Administrative_Duration'])\n",
    "# #scaled_datasets['7'] = X8_scaled.drop(columns = ['Informational'])\n",
    "# #scaled_datasets['6'] = X7_scaled.drop(columns = ['VisitorType'])\n",
    "# #scaled_datasets['5'] = X6_scaled.drop(columns = ['Administrative'])\n",
    "# #scaled_datasets['4'] = X5_scaled.drop(columns = ['BounceRates'])\n",
    "# #scaled_datasets['3'] = X4_scaled.drop(columns = ['ProductRelated_Duration'])\n",
    "# #scaled_datasets['2'] = X3_scaled.drop(columns = ['ProductRelated'])\n",
    "# #scaled_datasets['1'] = X2_scaled.drop(columns = ['ExitRates'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #tsne_plot(X_scaled, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of features: 17\n",
      "Accuracy : 0.8722627737226277\n",
      "Confusion Matrix: \n",
      "[[2039   26]\n",
      " [ 289  112]]\n",
      "True negatives: 2039\n",
      "False positives: 289\n",
      "False negatives: 26\n",
      "True positives: 112\n",
      "\n",
      "amount of features: 10\n",
      "Accuracy : 0.8811841038118411\n",
      "Confusion Matrix: \n",
      "[[2030   35]\n",
      " [ 258  143]]\n",
      "True negatives: 2030\n",
      "False positives: 258\n",
      "False negatives: 35\n",
      "True positives: 143\n",
      "\n",
      "amount of features: 9\n",
      "Accuracy : 0.8815896188158961\n",
      "Confusion Matrix: \n",
      "[[2029   36]\n",
      " [ 256  145]]\n",
      "True negatives: 2029\n",
      "False positives: 256\n",
      "False negatives: 36\n",
      "True positives: 145\n",
      "\n",
      "amount of features: 8\n",
      "Accuracy : 0.8819951338199513\n",
      "Confusion Matrix: \n",
      "[[2030   35]\n",
      " [ 256  145]]\n",
      "True negatives: 2030\n",
      "False positives: 256\n",
      "False negatives: 35\n",
      "True positives: 145\n",
      "\n",
      "amount of features: 7\n",
      "Accuracy : 0.8832116788321168\n",
      "Confusion Matrix: \n",
      "[[2031   34]\n",
      " [ 254  147]]\n",
      "True negatives: 2031\n",
      "False positives: 254\n",
      "False negatives: 34\n",
      "True positives: 147\n",
      "\n",
      "amount of features: 6\n",
      "Accuracy : 0.8860502838605029\n",
      "Confusion Matrix: \n",
      "[[2029   36]\n",
      " [ 245  156]]\n",
      "True negatives: 2029\n",
      "False positives: 245\n",
      "False negatives: 36\n",
      "True positives: 156\n",
      "\n",
      "amount of features: 5\n",
      "Accuracy : 0.8880778588807786\n",
      "Confusion Matrix: \n",
      "[[2021   44]\n",
      " [ 232  169]]\n",
      "True negatives: 2021\n",
      "False positives: 232\n",
      "False negatives: 44\n",
      "True positives: 169\n",
      "\n",
      "amount of features: 4\n",
      "Accuracy : 0.889294403892944\n",
      "Confusion Matrix: \n",
      "[[2012   53]\n",
      " [ 220  181]]\n",
      "True negatives: 2012\n",
      "False positives: 220\n",
      "False negatives: 53\n",
      "True positives: 181\n",
      "\n",
      "amount of features: 3\n",
      "Accuracy : 0.889294403892944\n",
      "Confusion Matrix: \n",
      "[[2008   57]\n",
      " [ 216  185]]\n",
      "True negatives: 2008\n",
      "False positives: 216\n",
      "False negatives: 57\n",
      "True positives: 185\n",
      "\n",
      "amount of features: 2\n",
      "Accuracy : 0.8888888888888888\n",
      "Confusion Matrix: \n",
      "[[1998   67]\n",
      " [ 207  194]]\n",
      "True negatives: 1998\n",
      "False positives: 207\n",
      "False negatives: 67\n",
      "True positives: 194\n",
      "\n",
      "amount of features: 1\n",
      "Accuracy : 0.8868613138686131\n",
      "Confusion Matrix: \n",
      "[[1978   87]\n",
      " [ 192  209]]\n",
      "True negatives: 1978\n",
      "False positives: 192\n",
      "False negatives: 87\n",
      "True positives: 209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "\n",
    "# Splitting dataset into train set and test set.\n",
    "def makepredictions(datasets):\n",
    "    \n",
    "    for dataset in datasets:\n",
    "    \n",
    "     #   X_train, X_test, y_train, y_test = train_test_split(datasets[dataset], Y, test_size = 0.2, random_state = 32342) \n",
    "\n",
    "\n",
    "        svmclf = SVC() \n",
    "        svmclf.fit(datasets[dataset][\"X_train\"], y_train) \n",
    "\n",
    "        y_pred_svmclf = svmclf.predict(datasets[dataset][\"X_test\"]) \n",
    "  \n",
    "        # Performance\n",
    "        print('amount of features: ' + dataset)\n",
    "        print('Accuracy : '+str(accuracy_score(y_test, y_pred_svmclf))) \n",
    "        print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_svmclf)))\n",
    "        # incorrect order of output\n",
    "#         tn, fn, fp, tp = confusion_matrix(y_test, y_pred_svmclf).ravel()\n",
    "        # the right order is as follows\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_svmclf).ravel()\n",
    "    \n",
    "        print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n",
    "makepredictions(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7891 samples, validate on 1973 samples\n",
      "Epoch 1/10\n",
      "7891/7891 [==============================] - 8s 952us/step - loss: 0.0078 - val_loss: 0.0059\n",
      "Epoch 2/10\n",
      "7891/7891 [==============================] - 7s 931us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 3/10\n",
      "7891/7891 [==============================] - 7s 909us/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 4/10\n",
      "7891/7891 [==============================] - 7s 932us/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 5/10\n",
      "7891/7891 [==============================] - 7s 914us/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 6/10\n",
      "7891/7891 [==============================] - 7s 931us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 7/10\n",
      "7891/7891 [==============================] - 8s 960us/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 8/10\n",
      "7891/7891 [==============================] - 8s 958us/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 9/10\n",
      "7891/7891 [==============================] - 7s 943us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "7891/7891 [==============================] - 8s 972us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "amount of features: \n",
      "Accuracy : 0.83779399837794\n",
      "Confusion Matrix: \n",
      "[[2065    0]\n",
      " [ 400    1]]\n",
      "True negatives: 2065\n",
      "False positives: 400\n",
      "False negatives: 0\n",
      "True positives: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense \n",
    "from keras.models import Model, Sequential \n",
    "from keras import regularizers \n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras import backend as K\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#X_scaled = scaler.fit_transform(X)\n",
    "#X_nonbuyer_scaled = X_scaled[Y == 0] \n",
    "#X_buyer_scaled = X_scaled[Y == 1]\n",
    "\n",
    "\n",
    "input_layer = Input(shape =(X.shape[1], )) \n",
    "\n",
    "#Encoder network Deep\n",
    "encoded_deep = Dense(100, activation ='tanh')(input_layer) \n",
    "encoded_deep = Dense(50, activation ='tanh')(encoded_deep) \n",
    "encoded_deep = Dense(25, activation ='tanh')(encoded_deep) \n",
    "encoded_deep = Dense(12, activation ='tanh')(encoded_deep) \n",
    "encoded_deep = Dense(6, activation ='relu')(encoded_deep) \n",
    "\n",
    "#Decoder network deep\n",
    "decoded_deep = Dense(12, activation ='tanh')(encoded_deep) \n",
    "decoded_deep = Dense(25, activation ='tanh')(decoded_deep) \n",
    "decoded_deep = Dense(50, activation ='tanh')(decoded_deep) \n",
    "decoded_deep = Dense(100, activation ='tanh')(decoded_deep) \n",
    "\n",
    "output_layer_deep = Dense(X.shape[1], activation ='relu')(decoded_deep) \n",
    "\n",
    "\n",
    "\n",
    "    # Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer_deep) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "\n",
    "    # Training the Auto-encoder network \n",
    "autoencoder.fit(datasets[\"17\"][\"X_train\"], datasets[\"17\"][\"X_train\"],  \n",
    "                    batch_size = 1, epochs = 10,  \n",
    "                    shuffle = True, validation_split = 0.2) \n",
    "\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "    # Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "    #nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "    #buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "\n",
    "    # Combining the encoded points into a single table  \n",
    "    #encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "    #y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "    #y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "    #encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "encoded_X_train = hidden_representation.predict(datasets[\"17\"][\"X_train\"]) \n",
    "\n",
    "encoded_X_test = hidden_representation.predict(datasets[\"17\"][\"X_test\"])\n",
    "\n",
    "\n",
    "classifier = SVC() \n",
    "classifier.fit(encoded_X_train, y_train) \n",
    "\n",
    "    # Storing the predictions of the linear model \n",
    "y_pred_classifier = classifier.predict(encoded_X_test) \n",
    "\n",
    "\n",
    "    # Plotting the encoded points \n",
    "    #tsne_plot(encoded_X, encoded_y) \n",
    "\n",
    "\n",
    "    # Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test, y_pred_classifier))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_classifier)))\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, y_pred_classifier).ravel()\n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7891 samples, validate on 1973 samples\n",
      "Epoch 1/7\n",
      "7891/7891 [==============================] - 0s 49us/step - loss: 6.6220 - val_loss: 4.2390\n",
      "Epoch 2/7\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 5.6310 - val_loss: 3.7217\n",
      "Epoch 3/7\n",
      "7891/7891 [==============================] - 0s 5us/step - loss: 4.9941 - val_loss: 3.3590\n",
      "Epoch 4/7\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 4.5488 - val_loss: 3.0926\n",
      "Epoch 5/7\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 4.2198 - val_loss: 2.8748\n",
      "Epoch 6/7\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 3.9388 - val_loss: 2.6991\n",
      "Epoch 7/7\n",
      "7891/7891 [==============================] - 0s 4us/step - loss: 3.7147 - val_loss: 2.5390\n",
      "amount of features: \n",
      "Accuracy : 0.8665855636658556\n",
      "Confusion Matrix: \n",
      "[[2059    6]\n",
      " [ 323   78]]\n",
      "True negatives: 2059\n",
      "False positives: 323\n",
      "False negatives: 6\n",
      "True positives: 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer_sparse = Input(shape =(X.shape[1], )) \n",
    "\n",
    "#Encoder network Sparsity constraint\n",
    "encoded_sparse = Dense(100, activation ='tanh',\n",
    "activity_regularizer = regularizers.l1(10e-5))(input_layer_sparse) \n",
    "encoded_sparse = Dense(50, activation ='tanh',\n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded_sparse) \n",
    "encoded_sparse = Dense(25, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded_sparse) \n",
    "encoded_sparse = Dense(12, activation ='tanh', \n",
    "activity_regularizer = regularizers.l1(10e-5))(encoded_sparse) \n",
    "encoded_sparse = Dense(6, activation ='relu')(encoded_sparse) \n",
    "\n",
    "#Decoder network Sparsity constraint\n",
    "decoded_sparse = Dense(12, activation ='tanh')(encoded_sparse) \n",
    "decoded_sparse = Dense(25, activation ='tanh')(decoded_sparse) \n",
    "decoded_sparse = Dense(50, activation ='tanh')(decoded_sparse) \n",
    "decoded_sparse = Dense(100, activation ='tanh')(decoded_sparse) \n",
    "\n",
    "output_layer_sparse = Dense(X.shape[1], activation ='relu')(decoded_sparse) \n",
    "\n",
    "\n",
    "\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer_sparse, output_layer_sparse) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "\n",
    "# Training the Auto-encoder network \n",
    "autoencoder.fit(datasets[\"17\"][\"X_train\"], datasets[\"17\"][\"X_train\"],  \n",
    "                    batch_size = 3000, epochs = 7,  \n",
    "                    shuffle = True, validation_split = 0.2) \n",
    "\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "    # Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "    #nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "    #buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "\n",
    "    # Combining the encoded points into a single table  \n",
    "    #encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "    #y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "    #y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "    #encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "encoded_X_train = hidden_representation.predict(datasets[\"17\"][\"X_train\"]) \n",
    "\n",
    "encoded_X_test = hidden_representation.predict(datasets[\"17\"][\"X_test\"])\n",
    "\n",
    "\n",
    "classifier = SVC() \n",
    "classifier.fit(encoded_X_train, y_train) \n",
    "\n",
    "    # Storing the predictions of the linear model \n",
    "y_pred_classifier = classifier.predict(encoded_X_test) \n",
    "\n",
    "\n",
    "    # Plotting the encoded points \n",
    "    #tsne_plot(encoded_X, encoded_y) \n",
    "\n",
    "\n",
    "    # Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test, y_pred_classifier))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_classifier)))\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, y_pred_classifier).ravel()\n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cb6513ed5cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Encoder network Convolutional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "#Encoder network Convolutional\n",
    "input_layer = Input(shape =(X.shape[1], )) \n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded_conv = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded_conv)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded_conv = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "\n",
    "    # Training the Auto-encoder network \n",
    "autoencoder.fit(datasets[\"17\"][\"X_train\"], datasets[\"17\"][\"X_train\"],  \n",
    "                    batch_size = 3000, epochs = 10,  \n",
    "                    shuffle = True, validation_split = 0.2) \n",
    "\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "    # Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "    #nonbuyer_hidden_rep = hidden_representation.predict(X_nonbuyer_scaled) \n",
    "    #buyer_hidden_rep = hidden_representation.predict(X_buyer_scaled) \n",
    "\n",
    "    # Combining the encoded points into a single table  \n",
    "    #encoded_X = np.append(nonbuyer_hidden_rep, buyer_hidden_rep, axis = 0) \n",
    "    #y_nonbuyer = np.zeros(nonbuyer_hidden_rep.shape[0]) \n",
    "    #y_buyer = np.ones(buyer_hidden_rep.shape[0]) \n",
    "    #encoded_y = np.append(y_nonbuyer, y_buyer) \n",
    "encoded_X_train = hidden_representation.predict(datasets[\"17\"][\"X_train\"]) \n",
    "\n",
    "encoded_X_test = hidden_representation.predict(datasets[\"17\"][\"X_test\"])\n",
    "\n",
    "\n",
    "classifier = SVC() \n",
    "classifier.fit(encoded_X_train, y_train) \n",
    "\n",
    "    # Storing the predictions of the linear model \n",
    "y_pred_classifier = classifier.predict(encoded_X_test) \n",
    "\n",
    "\n",
    "    # Plotting the encoded points \n",
    "    #tsne_plot(encoded_X, encoded_y) \n",
    "\n",
    "\n",
    "    # Performance\n",
    "print('amount of features: ')\n",
    "print('Accuracy : '+str(accuracy_score(y_test, y_pred_classifier))) \n",
    "print('Confusion Matrix: \\n' + str(confusion_matrix(y_test,y_pred_classifier)))\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, y_pred_classifier).ravel()\n",
    "print('True negatives: ' + str(tn) + '\\n' + 'False positives: ' + str(fp) +  '\\n' + 'False negatives: ' + str(fn) + '\\n'+ 'True positives: ' + str(tp) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
